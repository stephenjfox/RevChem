# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_tobii_resolve.ipynb.

# %% auto 0
__all__ = ['T_source', 'T_out', 'read_tobii_individual_tsv', 'tobii_timestamp_to_datetime', 'itersize',
           'enhance_realeye_metadata', 'read_realeye_raw_gazes_csv', 'RealEyeStruct',
           'unroll_realeye_dataframe_into_record_dataframes', 'list_concat', 'apply', 'clean_tsv_file_name',
           'filter_to_newyear_and_sort_by_timestamp', 'filter_tobii_dfs_by_new_years_heuristics',
           'filter_realeye_dfs_by_new_years_heuristics', 'generate_random_datetimes', 'find_tobii_realeye_df_pairs',
           'export_ordered_pairs', 'Resettable', 'iter_parse_raw_to_GazeInfo', 'resettable_iter_raw', 'RealEyeRawRow',
           'cumulative_sum', 'raw_gazes_row_to_df', 'run_realeye_df_group_statistics', 'realeye_timestamp_to_datetime',
           'correct_realeye_df_group', 'pipeline_raw_realeye_to_timed_dataframe']

# %% ../nbs/01_tobii_resolve.ipynb 2
import polars as pl

# %% ../nbs/01_tobii_resolve.ipynb 13
from .common import group_by

# %% ../nbs/01_tobii_resolve.ipynb 19
def read_tobii_individual_tsv(
    path_to_tsv: str,
    schema_overrides: dict[str, pl.DataType] = {},
    *,
    change_timestamp_to_datetime: bool = True,  # whether to use this record's derived datetime to calculate per-observation datetimestamps in "Recording timestamp"
) -> pl.DataFrame:
    "Reads a trial-exported Tobii TSV, with the minimal columns that we need"
    individual_tsv = pl.read_csv(
        path_to_tsv,
        separator="\t",
        infer_schema=True,
        schema_overrides={
            "Recording start time UTC": pl.Time,
            "Recording start time": pl.Time,
            "Gaze point X": pl.Int32,
            "Gaze point Y": pl.Int32,
        }
        | schema_overrides,
    )
    start_date_utc = pl.col("Recording date UTC").str.to_date("%m/%d/%Y")
    start_time_utc = pl.col("Recording start time UTC")

    start_dt_utc = (
        start_date_utc.dt.combine(
            pl.time(
                start_time_utc.dt.hour(),
                start_time_utc.dt.minute(),
                start_time_utc.dt.second(),
                start_time_utc.dt.microsecond(),
            )
        )
        .dt.replace_time_zone(time_zone="UTC")
        .alias("start_dt_utc")
    )
    start_time_ = pl.col("Recording start time")

    start_date_ = (
        pl.col("Recording date").str.to_date("%m/%d/%Y")
        if "Recording date" in individual_tsv.columns
        else (
            start_dt_utc.dt.convert_time_zone(time_zone="America/New_York")
            .dt.date()
            .alias("Recording date")
        )
    )
    start_dt_tz = (
        start_date_.dt.combine(
            pl.time(
                start_time_.dt.hour(),
                start_time_.dt.minute(),
                start_time_.dt.second(),
                start_time_.dt.microsecond(),
            )
        )
        .dt.replace_time_zone(time_zone="America/New_York")
        .alias("start_dt_tz")
    )

    result = individual_tsv.with_columns(
        start_date_utc,
        start_date_,
        start_dt_utc,
        start_dt_tz,
    )
    if change_timestamp_to_datetime:
        result = result.with_columns(  # separate call so the columns that we fiddle are guaranteed present
            tobii_timestamp_to_datetime(datetime_col="start_dt_utc")
        )
    # display(plan) when using a LazyFrame to get debugging support
    return result

# %% ../nbs/01_tobii_resolve.ipynb 21
def tobii_timestamp_to_datetime(
    datetime_col: str = "Recording start datetime UTC", # column with the recording start datetime, UTC or TZ-specific.
    timestamp_col: str = "Recording timestamp", # the integer column representing the microseconds since the recording started
    *,
    overwrite: bool = True, # Whether, the `timestamp_col` will have a datetime type in the result, or (if False) a new column is created
) -> pl.DataFrame:
    """Update `timestamp_col` to be an increasing datetime rather than the default (i64 or int).

    Corrects the `timestamp_col` of `df` to be a `pl.Datetime`, to ease legibility and computation.
    Sums the `timestamp_col` with that of the reference `datetime_col`, incrementing the time forward.

    Returns:
        A dataframe with the described change to the `timestamp_col`
    """
    new_name = timestamp_col if overwrite else f"{timestamp_col}__new_dt_time"
    new_column = pl.col(datetime_col) + pl.duration(microseconds=timestamp_col)
    new_column = new_column.alias(new_name)

    return new_column

# %% ../nbs/01_tobii_resolve.ipynb 37
from .realeye import iter_parse_raw_data

# %% ../nbs/01_tobii_resolve.ipynb 39
from typing import Iterable
def itersize(any_iter: Iterable) -> int:
    count = 0
    for _ in any_iter: count += 1
    return count

# %% ../nbs/01_tobii_resolve.ipynb 40
def enhance_realeye_metadata(realeye_df: pl.DataFrame) -> pl.DataFrame:
    "Decorate the metadata with addition information"
    # use pl.Struct to dynamically instruct the strong typing in Rust (which drives Polars)
    re_coord_type = pl.Struct({"gaze_point_x": pl.Int64, "gaze_point_y": pl.Int64})
    return realeye_df.with_columns(
        # unpack into dictionaries that function as tuples. Call them coordinate pairs
        coordinate_pairs=pl.col("test_raw_data").map_elements(
            lambda s: [
                {"gaze_point_x": sextuple[0], "gaze_point_y": sextuple[1]}
                for sextuple in iter_parse_raw_data(s)
            ],
            return_dtype=pl.List(re_coord_type),
        ),
        # count the len of the list of tuples RealEye exported into that one row. Call it n_elements
        n_elements=pl.col("test_raw_data").map_elements(
            lambda s: itersize(iter_parse_raw_data(s)), return_dtype=pl.Int32
        ),  # call that the number of elements
    ).drop("test_raw_data")
    # .sort(by="test_created_at") # temporal sort is not necessary. We've sorted the data already.


def read_realeye_raw_gazes_csv(
    path_to_csv: str | Path, decorated: bool = True
) -> pl.DataFrame:
    "Real the RealEye raw-gazes.csv, decorating with field of RealEyeStruct if `decorated` == True"
    raw_csv = pl.read_csv(
        path_to_csv,
        columns=["participant_id", "item_id", "test_created_at", "test_raw_data"],
        schema_overrides={"test_created_at": pl.Datetime},
    ).sort(by="test_created_at")

    if decorated:
        return enhance_realeye_metadata(raw_csv)
    else:
        return raw_csv

# %% ../nbs/01_tobii_resolve.ipynb 43
from typing import NamedTuple
from datetime import datetime

class RealEyeStruct(NamedTuple):
    "Truncated struct containing the minimum necessary information to make RealEye exports useful"
    participant_id: str # the participant being tested
    item_id: str # the stimulus being shown
    test_created_at: datetime # when the web browser started running (NOT THE SAME AS THE STIMULUS START TIME)
    coordinate_pairs: list[dict[str, int]] # [{"gaze_point_x": <int>, "gaze_point_y": <int>}]
    n_elements: int # number of coordinate pairs in <self>.coordinate_pairs

    @classmethod
    def from_tuple(cls, tuple_) -> 'RealEyeStruct':
        return cls(*tuple_)

# %% ../nbs/01_tobii_resolve.ipynb 44
from datetime import timedelta, datetime, UTC

def unroll_realeye_dataframe_into_record_dataframes(df: pl.DataFrame):
    """Convert each row of a RealEye-exported CSV into a dataframe of timestamped records
    
    We assume 30 Hz data is given, and so concatenate all dataframes in the order they are ingested/converted
    with a rolling 1/30th of a second added to the time of the first record encountered.
    """
    time_inc = timedelta(seconds=1/30) # 30 Hz data
    dfs = []
    for row in map(RealEyeStruct.from_tuple, df.rows()):
        pseudo_start_time = row.test_created_at # .replace(second=0)
        row_df = pl.DataFrame({
            "timestamp": row.test_created_at.replace(tzinfo=UTC), # we assert the RealEye dataframe is UTC timestamped, per the docs
            "X": [pairs["gaze_point_x"] for pairs in row.coordinate_pairs],
            "Y": [pairs["gaze_point_y"] for pairs in row.coordinate_pairs]
        })
        dfs.append((pseudo_start_time,row_df))

    # group by -> concat all the columns -> count the time with all the data in order
    # NOTE: grouping unique to the minute, which should be shared among RealEye, though the second may differ
    # in particular, several entries are exactly 1 second apart.
    # We assume the later of these entries is "next" chronologically.
    grouped_dfs = group_by(lambda tup: tup[0].replace(second=0, microsecond=0), dfs)
    output_dfs = []
    for start_time, group in grouped_dfs.items():
        df_at_start_time = pl.concat([df for _, df in group])
        start_time = df_at_start_time["timestamp"][0]
        end_time = start_time + time_inc * (df_at_start_time.shape[0] - 1)
        df_with_time_corrected = df_at_start_time.with_columns(
            timestamp=pl.datetime_range(start_time, end_time, time_inc)
        )
        output_dfs.append(df_with_time_corrected)
    
    return output_dfs

# %% ../nbs/01_tobii_resolve.ipynb 45
# from RevChem.common import list_concat
def list_concat(lists: list[list]) -> list:
    result = [
        item for l_ in lists for item in l_
    ]
    return result

# %% ../nbs/01_tobii_resolve.ipynb 51
def apply(s, transform): return transform(s)

def clean_tsv_file_name(fname: str) -> str:
    from functools import reduce

    transformations = [
        lambda s: s.split("1.Realeye1,2,3")[1].strip(),
        lambda s: s.rstrip(".tsv")
    ]

    return reduce(apply, transformations, fname)

# %% ../nbs/01_tobii_resolve.ipynb 56
from datetime import tzinfo, UTC
from typing import TypeVar
from .common import partition, Predicate


def filter_to_newyear_and_sort_by_timestamp(
    dfs: list[pl.DataFrame],
) -> list[pl.DataFrame]:
    return sorted(
        filter(
            lambda df: (df["timestamp"][0] >= datetime(2025, 1, 1, tzinfo=UTC)),
            dfs,
        ),
        key=lambda df: df["timestamp"][0],
    )


_T = TypeVar("_T")
type _Criterion[_T] = tuple[Predicate[_T], str]


def _filter_by_criteria_loop(
    dfs: list[pl.DataFrame],
    criteria: list[_Criterion],
    verbose: bool = True,
) -> tuple[list[pl.DataFrame], list[list[pl.DataFrame]]]:

    misses_in_order = []
    for criterion_index, (crit, explanation) in enumerate(criteria):
        dfs, missed = partition(crit, dfs)
        if missed:
            print(f'{len(missed)} dfs failed the {criterion_index}-th criterion "{explanation}"')
            if verbose:
                print(
                    f"The following previews show the dfs that missed the criteria: {explanation}"
                )
                with pl.Config(tbl_rows=10):
                    for miss_df in missed:
                        print(miss_df.head(2).with_columns(count=len(miss_df)))
        misses_in_order.append(missed)

    return dfs, misses_in_order


def filter_tobii_dfs_by_new_years_heuristics(
    dfs: list[pl.DataFrame],
) -> list[pl.DataFrame]:
    """
    Remove dataframes that do not meet our heuristic and discovered criteria for a valid Tobii trial.

    Returns:
        All of the DataFrames that meet all filtering heuristics and invariants
    """
    criteria = [
        (
            (lambda df: len(df) >= 51_000),
            "len(df) >= 51k, corresponding to >= 7 minutes of recording @ 120-HZ",
        ),
        (
            (lambda df: all((name not in df["source_tsv"][0] for name in ["Pichu2"]))),
            "found df in black list should not be test recordings",
        ),
    ]
    # If we ever need to do hard debugging, we can name that _ and inspect. Here, we're just releasing the memory.
    dfs, _ = _filter_by_criteria_loop(dfs, criteria)

    return dfs


def filter_realeye_dfs_by_new_years_heuristics(
    dfs: list[pl.DataFrame],
) -> list[pl.DataFrame]:
    """
    Remove dataframes that do not meet our heuristic and discovered criteria for a valid Tobii trial.

    Returns:
        All of the DataFrames that meet all filtering heuristics and invariants
    """
    criteria = [
        (
            (lambda df: len(df) >= 5_000),
            "len(df) >= 5k, corresponding to >= 2.8 minutes of recording @ 30-HZ",
        ),
    ]
    # If we ever need to do hard debugging, we can name that _ and inspect. Here, we're just releasing the memory.
    dfs, _ = _filter_by_criteria_loop(dfs, criteria)

    return dfs

# %% ../nbs/01_tobii_resolve.ipynb 62
from datetime import datetime, timedelta
import random

def generate_random_datetimes(start, count, max_minutes_range):
    """
    # Function to generate random datetime list
    # Parameters: start datetime, number of datetimes, max minutes range
    """
    return [start + timedelta(minutes=random.randint(0, max_minutes_range)) for _ in range(count)]


# %% ../nbs/01_tobii_resolve.ipynb 64
def find_tobii_realeye_df_pairs(
    tobii_dfs: list[pl.DataFrame], realeye_dfs: list[pl.DataFrame],
    *,
    _logging: bool = False
) -> list[tuple[pl.DataFrame, pl.DataFrame]]:
    """
    Algorithm

    0. Prep: dfs -> tuples of starting time and df, to ease the comparison function
        - alt: write a comparator that accesses that property
    1. Brutal force
        1. For each tobii session, find the closest RealEye session
        2. For logging purposes, articulate the `pl.Duration` or `datetime.TimeDelta`: `end - start` for RealEye
    2. More intelligent
        1. Pop found indices from the available indices to search, so you don't double-select a `DataFrame`
        2. Use the `source_tsv` property as a key into a dictionary, to store the found DataFrame once its found
            - or its index. Doesn't really matter
    """
    from datetime import datetime, timedelta
    times_tobii = [df["timestamp"][0] for df in tobii_dfs]
    times_realeye = [df["timestamp"][0] for df in realeye_dfs]
    # tobii is the reference, realeye is under scrutiny
    # 50 sec is the shortest time between Tobii "Start record", skipping validation, and starting RealEye
    _MIN_TIME_DELTA = timedelta(seconds=50)
    found_indices: set[int] = set() # just because I don't want to order it
    pair_indices = []
    for tobii_time in times_tobii:
        current_min_time_diff = timedelta(days=1_000) # more time than is sensible
        found_index = -1
        for i, re_time in enumerate(times_realeye):
            latest_diff = re_time - tobii_time
            if (i not in found_indices) and (_MIN_TIME_DELTA <= latest_diff < current_min_time_diff):
                found_index = i # we've found it. Don't need to log it
                current_min_time_diff = latest_diff
                _logging and print(f"Changed: {current_min_time_diff = }")
        found_indices.add(found_index)
        pair_indices.append(found_index)

    result = [
        (tobii_df.sort("timestamp"), realeye_dfs[re_index].sort("timestamp")) 
        for re_index, tobii_df in zip(pair_indices, tobii_dfs)
    ]
    return result

# %% ../nbs/01_tobii_resolve.ipynb 68
from .common import dt_str_now, date_str_now

# %% ../nbs/01_tobii_resolve.ipynb 69
def export_ordered_pairs(
    df_pairs: list[tuple[pl.DataFrame, pl.DataFrame]],
    *,
    output_dir_root: Path,
    output_suffix: Path,
    include_joined_output: bool = False,
    tobii_export_naming: dict[str, str] = {
        "timestamp": "timestamp_tobii",
        "X": "X_tobii",
        "Y": "Y_tobii",
    },
    exported_column_names=[
        "timestamp",
        "X",
        "Y",
    ],
):
    # exported_column_names += list(sorted(tobii_export_naming.values()))
    export_dir = output_dir_root / output_suffix
    for tobii_df, re_df in df_pairs:
        # name the directory after the trial date + subject
        trial_subject_date_and_name = tobii_df["source_tsv"][0]
        try:
            outputdir = export_dir / trial_subject_date_and_name
            outputdir.mkdir(exist_ok=False, parents=True)

            re_df[exported_column_names].write_csv(outputdir / "realeye.csv")
            tobii_df[exported_column_names].write_csv(outputdir / "tobii.csv")
            if include_joined_output:
                pl.concat(
                    [re_df, tobii_df.rename(tobii_export_naming)],
                    how="horizontal",
                ).write_csv(outputdir / "joined.csv")
        except Exception as e:
            print(f"Unexpected exception {e = }, {type(e) = }")
            print(f"{outputdir = }")

# %% ../nbs/01_tobii_resolve.ipynb 77
from typing import Iterator
from .realeye import GazeInfo, iter_parse_raw_data

T_source = TypeVar("T_source")
T_out = TypeVar("T_out")


class Resettable(Iterable[T_out]):
    def __init__(
        self, source_data: T_source, iter_gen: Callable[[T_source], Iterable[T_out]]
    ):
        self._source = source_data
        self.iter_gen = iter_gen

    def __iter__(self) -> Iterable[tuple[int, int, int, float, float, float]]:
        return self.iter_gen(self._source)


def iter_parse_raw_to_GazeInfo(raw_data: str) -> Iterator[GazeInfo]:
    for sextuple in iter_parse_raw_data(raw_data):
        if (length_ := len(sextuple)) > 6:
            # indicates that the RealEye system captured a mouse click. Nothing more.
            print(f"Got a {length_}-tuple: {sextuple = }")
        yield GazeInfo(*sextuple[:6])


def resettable_iter_raw(test_raw_data: str):
    return Resettable(test_raw_data, iter_parse_raw_to_GazeInfo)


@dataclass
class RealEyeRawRow:
    participant_id: str  # the participant being tested
    item_id: str  # the stimulus being shown
    test_created_at: datetime  # when the web browser started running (NOT THE SAME AS THE STIMULUS START TIME)
    raw_data: Iterable[GazeInfo] = None

    def __post_init__(self):
        self.raw_data = resettable_iter_raw(self.raw_data)

    @classmethod
    def from_row_tuples(cls, tuple) -> "RealEyeRawRow":
        return cls(*tuple)

# %% ../nbs/01_tobii_resolve.ipynb 81
def cumulative_sum(items: list[int|float]) -> list[int|float]:
    """Calculate the cumulative sum up to and including a given index"""
    csum = 0
    res = [None] * len(items)
    for i, item in enumerate(items):
        csum += item
        res[i] = csum
    return res

# %% ../nbs/01_tobii_resolve.ipynb 83
def raw_gazes_row_to_df(
    row: RealEyeRawRow, # typed row from the CSV. should have few, if any changes, from the raw CSV file. Used for semantic tidyness
    *,
    time_since_name: str = "time_since_start", # new name given to the column that records the time (ms) since this stimulus was shown
    x_name: str = "X", # new name given to the column that captures the X-coordinate of the GazeInfo gaze
    y_name: str = "Y", # new name given to the column that captures the Y-coordinate of the GazeInfo gaze
) -> pl.DataFrame:
    df = pl.DataFrame(
        [
            (
                row.test_created_at,
                gaze_info.time_ms_since_start,
                gaze_info.gaze_point_X,
                gaze_info.gaze_point_Y,
            )
            for gaze_info in row.raw_data
        ],
        schema={
            "test_created_at": pl.Datetime,
            time_since_name: pl.Int32,
            "X": pl.Int32,
            "Y": pl.Int32,
        },
        orient="row",
    )
    return df.with_columns( # force everything to be UTC because that's what it should be (per docs)
        pl.col("test_created_at").dt.replace_time_zone(time_zone="UTC"),
    )

# %% ../nbs/01_tobii_resolve.ipynb 84
def run_realeye_df_group_statistics(dfs: list[pl.DataFrame]):
    grouped = group_by(lambda df: df["test_created_at"][0], dfs)
    group_statistics = pl.DataFrame(
        [[group_key, len(group)] for group_key, group in grouped.items()],
        schema=["test_created_at", "n_rows"],
        orient="row",
    )  # .sort("test_created_at")
    with pl.Config(tbl_rows=50):
        display(group_statistics)
    group_statistics.write_csv(EXPORT_ROOT / f"{dt_str_now()}-row_stats.csv")

# %% ../nbs/01_tobii_resolve.ipynb 85
def realeye_timestamp_to_datetime(
    datetime_col: str = "test_created_at", # column with the recording start datetime
    timestamp_col: str = "time_ms_since_start", # the integer column representing the milliseconds since stimulus exposure
    *,
    overwrite: bool = True, # Whether, the `timestamp_col` will have a datetime type in the result, or (if False) a new column is created
    additional_offset_ms: int = 0, # Additional offset to 
) -> pl.DataFrame:
    """Update `timestamp_col` to be an increasing datetime rather than the default (i64 or int).

    Corrects the `timestamp_col` of `df` to be a `pl.Datetime`, to ease legibility and computation.
    Sums the `timestamp_col` with that of the reference `datetime_col`, incrementing the time forward.

    Returns:
        A dataframe with the described change to the `timestamp_col`
    """
    new_name = timestamp_col if overwrite else f"{timestamp_col}__new_dt_time"
    new_column = pl.col(datetime_col) + pl.duration(milliseconds=timestamp_col) + pl.duration(milliseconds=additional_offset_ms)
    new_column = new_column.alias(new_name)

    return new_column

# %% ../nbs/01_tobii_resolve.ipynb 86
def correct_realeye_df_group(
    group_dfs: list[pl.DataFrame], *, time_col: str = "time_since_start"
):
    """In-place mutation to correct the dfs' timing, assuming dfs are aggregated by `test_created_at`"""
    # 1. sum the last|largest millisecond offset from each of the dfs
    group_millisecond_offset_maxes = [df[time_col].max() for df in group_dfs]
    total_milliseconds_since_start = sum(group_millisecond_offset_maxes)
    # 1a. assume we have fully contiguous time series
    # 2. compute the start time: "test_created_at" - total relative milliseconds
    # Using min (not max), because the file outputs take time, even though the recording is done,
    # and the trial is started earlier than the one-ish sec it takes to create the output.
    re_recording_end = min(map(lambda df: df["test_created_at"][0], group_dfs))
    re_recording_start = re_recording_end - timedelta(
        microseconds=total_milliseconds_since_start * 1000
    )
    # 3. roll the relative milliseconds forward for each subsequent DataFrame
    # this is like a "scan" or "cummulative sum"
    # We shift everything "left" one, because the first doesn't need anything additional
    # The second df need only add the first, third df only add the two prior, etc.
    addend_group_millisecond_offsets = [0] + cumulative_sum(
        group_millisecond_offset_maxes[:-1]
    )
    # update the dfs
    for group_member_index in range(len(group_dfs)):
        df = group_dfs[group_member_index].with_columns(
            __temp_start_time=re_recording_start
        )
        group_dfs[group_member_index] = df.with_columns(
            realeye_timestamp_to_datetime(
                datetime_col="__temp_start_time",
                timestamp_col=time_col,
                additional_offset_ms=addend_group_millisecond_offsets[
                    group_member_index
                ],
            )
        ).drop("__temp_start_time")

# %% ../nbs/01_tobii_resolve.ipynb 87
# TODO: rename realeye data pipeline function
def pipeline_raw_realeye_to_timed_dataframe(
    re_raw_df: pl.DataFrame,  # result of pl.read_csv("raw-gazes.csv").
    *,
    do_group_stats_export: bool = False,  # whether compute early stats, write them to EXPORT_ROOT and exit early. Fails if EXPORT_ROOT is undefined.
    debug: bool = False,  # whether we output the first row of each dataframe, to debug what we're looking at.
    dt_timestamp_col: str = "time_since_start",  # name for the datetime timestamp column in the output dataframes
):
    real_eye_rows: list[RealEyeRawRow] = sorted(
        map(RealEyeRawRow.from_row_tuples, re_raw_df.rows()),
        # sorted by item_id, leveraging that list.index(...) -> ordinal position
        key=lambda re_row: item_ids_in_order_LIST.index(re_row.item_id),
    )
    # rows to dataframe
    dfs = [
        raw_gazes_row_to_df(row, time_since_name=dt_timestamp_col)
        for row in real_eye_rows
    ]

    if do_group_stats_export:
        run_realeye_df_group_statistics(dfs)
        return

    if debug:
        display(pl.concat([df.head(1) for df in dfs]))

    # group by "creation" time, down to the second, to get first ordering
    # then flatten so we have an overall sequence with subsequences which are in order.
    # Sorting is performed to make sure the test_created_at and test_created_at+1sec are in the correct order.
    dfs = list_concat(
        sorted(
            group_by(lambda df: df["test_created_at"][0], dfs).values(),
            key=lambda dfs: dfs[0]["test_created_at"][0],
        )
    )
    # group by the minute, order is retained within the group
    # giving us groups that put all entries of a given trial in the right order
    # even if split by a single second, they are collected in the correct stimulus order
    # and are results are output within a minute of each other.
    grouped = group_by(
        lambda df: df["test_created_at"][0].replace(second=0, microsecond=0), dfs
    )
    # now we can apply the timestamp correction algorithm to the groups
    for group_start_minute, group_dfs in grouped.items():
        correct_realeye_df_group(group_dfs, time_col=dt_timestamp_col)

    # lastly, concatenate all of the groups, now that their time columns are fixed
    mapped = {
        group_start_minute: pl.concat(group_dfs)
        for group_start_minute, group_dfs in grouped.items()
    }

    return mapped

