# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_tobii_resolve.ipynb.

# %% auto 0
__all__ = ['REALEYE_ITEM_IDS', 'COLUMNS_TOBII', 'COLUMN_RENAMING_REALEYE_TO_CSV', 'COLUMN_RENAMING_TOBII_TO_CSV', 'K', 'DF', 'R',
           'tobii_timestamp_to_datetime', 'read_tobii_individual_tsv', 'itersize', 'enhance_realeye_metadata',
           'read_realeye_raw_gazes_csv', 'RealEyeStruct', 'unroll_realeye_dataframe_into_record_dataframes',
           'GroupedFrames', 'count_group_backwards', 'unroll_realeye_df_counting_backwards', 'apply',
           'clean_tsv_file_name', 'filter_to_newyear_and_sort_by_timestamp', 'filter_tobii_dfs_by_new_years_heuristics',
           'filter_realeye_dfs_by_new_years_heuristics', 'generate_random_datetimes', 'find_tobii_realeye_df_pairs',
           'export_ordered_pairs']

# %% ../nbs/01_tobii_resolve.ipynb 2
import polars as pl
# from pathlib import Path

# %% ../nbs/01_tobii_resolve.ipynb 9
# the old code
# item_ids_in_order = raw_gazes_csv.unique("item_id", maintain_order=True)["item_id"]
REALEYE_ITEM_IDS = [
    "5b8637a5-c1c7-47cc-a1b4-5abe24c5b5aa",
    "83e23c46-4fe9-483b-abb5-4ba3cce6fd9e",
    "f456cacf-9abd-4825-a964-859f1b986c81",
    "d4bdad0c-13ae-4276-b1f4-007c2ad7a495",
    "e9978edb-9cdf-4db7-acef-3a08dfec2991",
    "75ee4147-26ff-48ff-8c88-1743c33b86c9",
    "63dc6493-df28-4aba-9d79-1dedc37d3bca",
    "efff95c6-ae5c-482a-9388-dafb0050dc3b",
    "e5762660-01a6-4e77-8e04-066b96d066c1",
    "b1641c81-54ff-4370-ab32-8476efce616d",
]

# %% ../nbs/01_tobii_resolve.ipynb 14
from .common import group_by

# %% ../nbs/01_tobii_resolve.ipynb 20
def tobii_timestamp_to_datetime(
    datetime_col: str = "Recording start datetime UTC", # column with the recording start datetime, UTC or TZ-specific.
    timestamp_col: str = "Recording timestamp", # the integer column representing the microseconds since the recording started
    *,
    overwrite: bool = True, # Whether, the `timestamp_col` will have a datetime type in the result, or (if False) a new column is created
) -> pl.DataFrame:
    """Update `timestamp_col` to be an increasing datetime rather than the default (i64 or int).

    Corrects the `timestamp_col` of `df` to be a `pl.Datetime`, to ease legibility and computation.
    Sums the `timestamp_col` with that of the reference `datetime_col`, incrementing the time forward.

    Returns:
        A dataframe with the described change to the `timestamp_col`
    """
    new_name = timestamp_col if overwrite else f"{timestamp_col}__new_dt_time"
    new_column = pl.col(datetime_col) + pl.duration(microseconds=timestamp_col)
    new_column = new_column.alias(new_name)

    return new_column

# %% ../nbs/01_tobii_resolve.ipynb 21
def read_tobii_individual_tsv(
    path_to_tsv: str,
    schema_overrides: dict[str, pl.DataType] = {},
    *,
    change_timestamp_to_datetime: bool = True,  # whether to use this record's derived datetime to calculate per-observation datetimestamps in "Recording timestamp"
) -> pl.DataFrame:
    "Reads a trial-exported Tobii TSV, with the minimal columns that we need"
    individual_tsv = pl.read_csv(
        path_to_tsv,
        separator="\t",
        infer_schema=True,
        schema_overrides={
            "Recording start time UTC": pl.Time,
            "Recording start time": pl.Time,
            "Gaze point X": pl.Int32,
            "Gaze point Y": pl.Int32,
        }
        | schema_overrides,
    )
    start_date_utc = pl.col("Recording date UTC").str.to_date("%m/%d/%Y")
    start_time_utc = pl.col("Recording start time UTC")

    start_dt_utc = (
        start_date_utc.dt.combine(
            pl.time(
                start_time_utc.dt.hour(),
                start_time_utc.dt.minute(),
                start_time_utc.dt.second(),
                start_time_utc.dt.microsecond(),
            )
        )
        .dt.replace_time_zone(time_zone="UTC")
        .alias("start_dt_utc")
    )
    start_time_ = pl.col("Recording start time")

    start_date_ = (
        pl.col("Recording date").str.to_date("%m/%d/%Y")
        if "Recording date" in individual_tsv.columns
        else (
            start_dt_utc.dt.convert_time_zone(time_zone="America/New_York")
            .dt.date()
            .alias("Recording date")
        )
    )
    start_dt_tz = (
        start_date_.dt.combine(
            pl.time(
                start_time_.dt.hour(),
                start_time_.dt.minute(),
                start_time_.dt.second(),
                start_time_.dt.microsecond(),
            )
        )
        .dt.replace_time_zone(time_zone="America/New_York")
        .alias("start_dt_tz")
    )

    result = individual_tsv.with_columns(
        start_date_utc,
        start_date_,
        start_dt_utc,
        start_dt_tz,
    )
    if change_timestamp_to_datetime:
        result = result.with_columns(  # separate call so the columns that we fiddle are guaranteed present
            tobii_timestamp_to_datetime(datetime_col="start_dt_utc")
        )
    # display(plan) when using a LazyFrame to get debugging support
    return result

# %% ../nbs/01_tobii_resolve.ipynb 29
COLUMNS_TOBII = [
    "Recording timestamp",
    "Gaze point X",
    "Gaze point Y",
]

# %% ../nbs/01_tobii_resolve.ipynb 30
from pathlib import Path

# %% ../nbs/01_tobii_resolve.ipynb 37
COLUMN_RENAMING_REALEYE_TO_CSV = {
    "timestamp": "timestamp",
    "gaze_point_x": "X",
    "gaze_point_y": "Y"
}
COLUMN_RENAMING_TOBII_TO_CSV = {
    "Recording timestamp": "timestamp",
    "Gaze point X": "X",
    "Gaze point Y": "Y"
}

# %% ../nbs/01_tobii_resolve.ipynb 38
from .realeye import iter_parse_raw_data

# %% ../nbs/01_tobii_resolve.ipynb 40
from typing import Iterable
def itersize(any_iter: Iterable) -> int:
    count = 0
    for _ in any_iter: count += 1
    return count

# %% ../nbs/01_tobii_resolve.ipynb 41
def enhance_realeye_metadata(realeye_df: pl.DataFrame) -> pl.DataFrame:
    "Decorate the RealEye metadata (typed or not) with addition information"
    # use pl.Struct to dynamically instruct the strong typing in Rust (which drives Polars)
    re_coord_type = pl.Struct({"gaze_point_x": pl.Int64, "gaze_point_y": pl.Int64})
    return realeye_df.with_columns(
        # unpack into dictionaries that function as tuples. Call them coordinate pairs
        coordinate_pairs=pl.col("test_raw_data").map_elements(
            lambda s: [
                {"gaze_point_x": sextuple[0], "gaze_point_y": sextuple[1]}
                for sextuple in iter_parse_raw_data(s)
            ],
            return_dtype=pl.List(re_coord_type),
        ),
        # count the len of the list of tuples RealEye exported into that one row. Call it n_elements
        n_elements=pl.col("test_raw_data").map_elements(
            lambda s: itersize(iter_parse_raw_data(s)), return_dtype=pl.Int32
        ),  # call that the number of elements
    ).drop("test_raw_data")
    # .sort(by="test_created_at") # temporal sort is not necessary. We've sorted the data already.


def read_realeye_raw_gazes_csv(
    path_to_csv: str | Path, decorated: bool = True
) -> pl.DataFrame:
    "Real the RealEye raw-gazes.csv, decorating with field of RealEyeStruct if `decorated` == True"
    raw_csv = pl.read_csv(
        path_to_csv,
        columns=["participant_id", "item_id", "test_created_at", "test_raw_data"],
        schema_overrides={"test_created_at": pl.Datetime},
    ).sort(by="test_created_at")

    if decorated:
        return enhance_realeye_metadata(raw_csv)
    else:
        return raw_csv

# %% ../nbs/01_tobii_resolve.ipynb 44
from typing import NamedTuple
from datetime import datetime

class RealEyeStruct(NamedTuple):
    "Truncated struct containing the minimum necessary information to make RealEye exports useful"
    participant_id: str # the participant being tested
    item_id: str # the stimulus being shown
    test_created_at: datetime # when the web browser started running (NOT THE SAME AS THE STIMULUS START TIME)
    coordinate_pairs: list[dict[str, int]] # [{"gaze_point_x": <int>, "gaze_point_y": <int>}]
    n_elements: int # number of coordinate pairs in <self>.coordinate_pairs

    @classmethod
    def from_tuple(cls, tuple_) -> 'RealEyeStruct':
        return cls(*tuple_)

# %% ../nbs/01_tobii_resolve.ipynb 45
from datetime import timedelta, datetime, UTC

def unroll_realeye_dataframe_into_record_dataframes(df: pl.DataFrame):
    """Convert each row of a RealEye-exported CSV into a dataframe of timestamped records
    
    We assume 30 Hz data is given, and so concatenate all dataframes in the order they are ingested/converted
    with a rolling 1/30th of a second added to the time of the first record encountered.
    """
    time_inc = timedelta(seconds=1/30) # 30 Hz data
    dfs = []
    for row in map(RealEyeStruct.from_tuple, df.rows()):
        pseudo_start_time = row.test_created_at # .replace(second=0)
        row_df = pl.DataFrame({
            "timestamp": row.test_created_at.replace(tzinfo=UTC), # we assert the RealEye dataframe is UTC timestamped, per the docs
            "X": [pairs["gaze_point_x"] for pairs in row.coordinate_pairs],
            "Y": [pairs["gaze_point_y"] for pairs in row.coordinate_pairs]
        })
        dfs.append((pseudo_start_time,row_df))

    # group by -> concat all the columns -> count the time with all the data in order
    # NOTE: grouping unique to the minute, which should be shared among RealEye, though the second may differ
    # in particular, several entries are exactly 1 second apart.
    # We assume the later of these entries is "next" chronologically.
    grouped_dfs = group_by(lambda tup: tup[0].replace(second=0, microsecond=0), dfs)
    output_dfs = []
    for start_time, group in grouped_dfs.items():
        df_at_start_time = pl.concat([df for _, df in group])
        start_time = df_at_start_time["timestamp"][0]
        end_time = start_time + time_inc * (df_at_start_time.shape[0] - 1)
        df_with_time_corrected = df_at_start_time.with_columns(
            timestamp=pl.datetime_range(start_time, end_time, time_inc)
        )
        output_dfs.append(df_with_time_corrected)
    
    return output_dfs

# %% ../nbs/01_tobii_resolve.ipynb 46
from .common import list_concat

# %% ../nbs/01_tobii_resolve.ipynb 47
import collections
from typing import (
    Callable,
    Dict,
    Generic,
    Iterable,
    List,
    Tuple,
    TypeVar,
)

# Define generic type variables for keys and DataFrames
K = TypeVar("K")  # Key type (e.g., datetime, str, int)
DF = TypeVar("DF", bound=pl.DataFrame)  # Value type, bound to Polars DataFrame
R = TypeVar("R")  # Return type for the apply method


class GroupedFrames(Generic[K, DF]):
    """
    A container for DataFrames grouped by a key.

    This class provides a more expressive and functional alternative to a raw `dict[key, list[DataFrame]]`.
    It is designed to hold groups of Polars DataFrames and offers a dedicated API for common aggregation and
    transformation tasks.

    The internal groups are stored in a dictionary sorted by the group key to ensure predictable iteration order.

    Args:
        data (Dict[K, List[DF]]): A dictionary mapping group keys to lists
                                  of DataFrames.
    """

    def __init__(self, data: Dict[K, List[DF]]):
        # Store data internally, ensuring it's sorted by key for consistency.
        self._groups: Dict[K, List[DF]] = dict(sorted(data.items()))

    @classmethod
    def from_tuples(cls, iterable: Iterable[Tuple[K, DF]]) -> "GroupedFrames[K, DF]":
        """
        Create a GroupedFrames instance from an iterable of (key, DataFrame) tuples.

        This factory method is the primary way to construct a GroupedFrames object.
        It performs the grouping operation itself.

        Args:
            iterable (Iterable[Tuple[K, DF]]): An iterable yielding tuples of (group_key, data_frame).

        Returns:
            GroupedFrames[K, DF]: A new instance with the data grouped by key.
        """
        groups = collections.defaultdict(list)
        for key, frame in iterable:
            groups[key].append(frame)
        return cls(dict(groups))

    def __repr__(self) -> str:
        """Provides a concise representation of the object."""
        num_groups = len(self._groups)
        # Preview the first 5 keys for context
        keys_preview = list(self._groups.keys())[:5]
        keys_str = ", ".join(map(str, keys_preview))
        if num_groups > 5:
            keys_str += ", ..."
        return f"{self.__class__.__name__}(num_groups={num_groups}, keys=[{keys_str}])"

    # --- Dictionary-like interface ---

    def __len__(self) -> int:
        """Returns the number of groups."""
        return len(self._groups)

    def __getitem__(self, key: K) -> List[DF]:
        """Retrieves the list of DataFrames for a given key."""
        return self._groups[key]

    def __iter__(self):
        """Iterates over the group keys."""
        return iter(self._groups)

    def keys(self):
        """Returns the view of group keys."""
        return self._groups.keys()

    def values(self):
        """Returns the view of the lists of DataFrames."""
        return self._groups.values()

    def items(self):
        """Returns a view of the (key, list[DataFrame]) items."""
        return self._groups.items()

    # --- Specialized Methods ---

    def ungroup(self) -> list[tuple[K, DF]]:
        """
        Flattens the groups into a single list of all DataFrames.

        The order of frames is preserved based on the sorted group keys and the original order of frames within each group.

        Returns:
            list[DF]: A single list containing all DataFrames with their keys from all groups.
        """
        return list_concat([(key, frame) for key, frames in self._groups.items() for frame in frames])

    def concat_groups(self, **kwargs) -> Dict[K, DF]:
        """
        Concatenates the DataFrames within each group into a single DataFrame.

        This is useful for consolidating fragmented data for each group key.
        Any keyword arguments are passed directly to `polars.concat`.

        Args:
            **kwargs: Additional keyword arguments for `pl.concat()`, e.g.,
                      `how='diagonal'`.

        Returns:
            Dict[K, DF]: A dictionary mapping each key to its single,
                         concatenated DataFrame.
        """
        return {
            key: pl.concat(frames, **kwargs) for key, frames in self.items() if frames
        }

    def apply(self, func: Callable[[K, list[DF]], R], **kwargs) -> Dict[K, R]:
        """
        Applies a function to each group and returns the results.

        The function receives the group key and the list of DataFrames for that group.

        Args:
            func (Callable[[K, list[DF]], R]): The function to apply.
                                               It takes `(key, frames)` and returns a result.
            **kwargs: Additional keyword arguments for `func`, e.g., `x=2` for `func(a, b, *, x=10)`.


        Returns:
            Dict[K, R]: A dictionary mapping each key to the result of the function.
        """
        return {key: func(key, frames) for key, frames in self.items()}

# %% ../nbs/01_tobii_resolve.ipynb 48
def count_group_backwards(
    end_time: datetime,
    group: Iterable[pl.DataFrame],
    *,
    time_inc: timedelta = timedelta(seconds=1 / 30),
) -> pl.DataFrame:
    # (old) debug stuff:
    # print(f"{type(group) = } {type(group[0]) = }")
    concatted_df = pl.concat([df for df in group])
    # print(f"{type(concatted_df) = }")
    end_time = end_time.replace(tzinfo=UTC)  # coerce to UTC
    # assert end_time == end_time_, f"{end_time} != {end_time_}"
    start_time = end_time - time_inc * (concatted_df.shape[0] - 1)

    # overwrites the timestamp column by making a new "timestamp" column with the same name
    df_with_time_corrected = concatted_df.with_columns(
        timestamp=pl.datetime_range(start_time, end_time, time_inc)
    )

    return df_with_time_corrected


def unroll_realeye_df_counting_backwards(df: pl.DataFrame):
    """Convert RealEye Raw record to the unrolled record, with the start_time interpreted as the end time.

    Video evidence suggests that the start_time field is actually more like the "recording completed at" time
    i.e. the end time.
    """
    time_inc = timedelta(seconds=1 / 30)  # 30 Hz data
    dfs_to_group = []
    for row in map(RealEyeStruct.from_tuple, df.rows()):
        # we assert the RealEye dataframe is UTC timestamped, per the docs
        # "timestamp": row.test_created_at.replace(tzinfo=UTC),
        row_df = pl.DataFrame(
            {
                "X": [pairs["gaze_point_x"] for pairs in row.coordinate_pairs],
                "Y": [pairs["gaze_point_y"] for pairs in row.coordinate_pairs],
            }
        )
        dfs_to_group.append((row.test_created_at, row_df))

    # group by -> concat all the columns -> count the time with all the data in order
    # NOTE: grouping unique to the minute, which should be shared among RealEye, though the second may differ
    grouped_dfs = GroupedFrames.from_tuples(dfs_to_group)

    # in particular, several entries are exactly 1 second apart.
    # We assume the later of these entries is "next" chronologically.
    grouped_dfs = GroupedFrames.from_tuples(
        (ts.replace(second=0, microsecond=0), df) for ts, group in grouped_dfs.items()
        for df in group
    )

    output_dfs = grouped_dfs.apply(count_group_backwards, time_inc=time_inc)

    # flatten out the group values, again
    return output_dfs


# %% ../nbs/01_tobii_resolve.ipynb 53
def apply(s, transform): return transform(s)

def clean_tsv_file_name(fname: str) -> str:
    from functools import reduce

    transformations = [
        lambda s: s.split("1.Realeye1,2,3")[1].strip(),
        lambda s: s.rstrip(".tsv")
    ]

    return reduce(apply, transformations, fname)

# %% ../nbs/01_tobii_resolve.ipynb 58
from datetime import tzinfo, UTC
from typing import TypeVar
from .common import partition, Predicate


def filter_to_newyear_and_sort_by_timestamp(
    dfs: list[pl.DataFrame],
) -> list[pl.DataFrame]:
    return sorted(
        filter(
            lambda df: (df["timestamp"][0] >= datetime(2025, 1, 1, tzinfo=UTC)),
            dfs,
        ),
        key=lambda df: df["timestamp"][0],
    )


_T = TypeVar("_T")
_Criterion = tuple[Predicate[_T], str]


def _filter_by_criteria_loop(
    dfs: list[pl.DataFrame],
    criteria: list[_Criterion],
    verbose: bool = True,
) -> tuple[list[pl.DataFrame], list[list[pl.DataFrame]]]:

    misses_in_order = []
    for criterion_index, (crit, explanation) in enumerate(criteria):
        dfs, missed = partition(crit, dfs)
        if missed:
            print(f'{len(missed)} dfs failed the {criterion_index}-th criterion "{explanation}"')
            if verbose:
                print(
                    f"The following previews show the dfs that missed the criteria: {explanation}"
                )
                with pl.Config(tbl_rows=10):
                    for miss_df in missed:
                        print(miss_df.head(2).with_columns(count=len(miss_df)))
        misses_in_order.append(missed)

    return dfs, misses_in_order


def filter_tobii_dfs_by_new_years_heuristics(
    dfs: list[pl.DataFrame],
) -> list[pl.DataFrame]:
    """
    Remove dataframes that do not meet our heuristic and discovered criteria for a valid Tobii trial.

    Returns:
        All of the DataFrames that meet all filtering heuristics and invariants
    """
    criteria = [
        (
            (lambda df: len(df) >= 51_000),
            "len(df) >= 51k, corresponding to >= 7 minutes of recording @ 120-HZ",
        ),
        (
            (lambda df: all((name not in df["source_tsv"][0] for name in ["Pichu2"]))),
            "found df in black list should not be test recordings",
        ),
    ]
    # If we ever need to do hard debugging, we can name that _ and inspect. Here, we're just releasing the memory.
    dfs, _ = _filter_by_criteria_loop(dfs, criteria)

    return dfs


def filter_realeye_dfs_by_new_years_heuristics(
    dfs: list[pl.DataFrame],
) -> list[pl.DataFrame]:
    """
    Remove dataframes that do not meet our heuristic and discovered criteria for a valid Tobii trial.

    Returns:
        All of the DataFrames that meet all filtering heuristics and invariants
    """
    criteria = [
        (
            (lambda df: len(df) >= 5_000),
            "len(df) >= 5k, corresponding to >= 2.8 minutes of recording @ 30-HZ",
        ),
    ]
    # If we ever need to do hard debugging, we can name that _ and inspect. Here, we're just releasing the memory.
    dfs, _ = _filter_by_criteria_loop(dfs, criteria)

    return dfs

# %% ../nbs/01_tobii_resolve.ipynb 64
from datetime import datetime, timedelta
import random

def generate_random_datetimes(start, count, max_minutes_range):
    """
    # Function to generate random datetime list
    # Parameters: start datetime, number of datetimes, max minutes range
    """
    return [start + timedelta(minutes=random.randint(0, max_minutes_range)) for _ in range(count)]


# %% ../nbs/01_tobii_resolve.ipynb 66
def find_tobii_realeye_df_pairs(
    tobii_dfs: list[pl.DataFrame], realeye_dfs: list[pl.DataFrame],
    *,
    _logging: bool = False
) -> list[tuple[pl.DataFrame, pl.DataFrame]]:
    """
    Algorithm

    0. Prep: dfs -> tuples of starting time and df, to ease the comparison function
        - alt: write a comparator that accesses that property
    1. Brutal force
        1. For each tobii session, find the closest RealEye session
        2. For logging purposes, articulate the `pl.Duration` or `datetime.TimeDelta`: `end - start` for RealEye
    2. More intelligent
        1. Pop found indices from the available indices to search, so you don't double-select a `DataFrame`
        2. Use the `source_tsv` property as a key into a dictionary, to store the found DataFrame once its found
            - or its index. Doesn't really matter
    """
    times_tobii = [df["timestamp"][0] for df in tobii_dfs]
    times_realeye = [df["timestamp"][0] for df in realeye_dfs]
    # tobii is the reference, realeye is under scrutiny
    # 50 sec is the shortest time between Tobii "Start record", skipping validation, and starting RealEye
    _MIN_TIME_DELTA = timedelta(seconds=50)
    found_indices: set[int] = set() # just because I don't want to order it
    pair_indices = []
    for tobii_time in times_tobii:
        current_min_time_diff = timedelta(days=1_000) # more time than is sensible
        found_index = -1
        for i, re_time in enumerate(times_realeye):
            latest_diff = re_time - tobii_time
            if (i not in found_indices) and (_MIN_TIME_DELTA <= latest_diff < current_min_time_diff):
                found_index = i # we've found it. Don't need to log it
                current_min_time_diff = latest_diff
                _logging and print(f"Changed: {current_min_time_diff = }")
        found_indices.add(found_index)
        pair_indices.append(found_index)

    result = [
        (tobii_df.sort("timestamp"), realeye_dfs[re_index].sort("timestamp")) 
        for re_index, tobii_df in zip(pair_indices, tobii_dfs)
    ]
    return result

# %% ../nbs/01_tobii_resolve.ipynb 70
from .common import dt_str_now, date_str_now

# %% ../nbs/01_tobii_resolve.ipynb 71
def export_ordered_pairs(
    df_pairs: list[tuple[pl.DataFrame, pl.DataFrame]],
    *,
    output_dir_root: Path,
    output_suffix: Path,
    include_joined_output: bool = False,
    tobii_export_naming: dict[str, str] = {
        "timestamp": "timestamp_tobii",
        "X": "X_tobii",
        "Y": "Y_tobii",
    },
    exported_column_names=[
        "timestamp",
        "X",
        "Y",
    ],
):
    # exported_column_names += list(sorted(tobii_export_naming.values()))
    export_dir = output_dir_root / output_suffix
    for tobii_df, re_df in df_pairs:
        # name the directory after the trial date + subject
        trial_subject_date_and_name = tobii_df["source_tsv"][0]
        try:
            outputdir = export_dir / trial_subject_date_and_name
            outputdir.mkdir(exist_ok=False, parents=True)

            re_df[exported_column_names].write_csv(outputdir / "realeye.csv")
            tobii_df[exported_column_names].write_csv(outputdir / "tobii.csv")
            if include_joined_output:
                pl.concat(
                    [re_df, tobii_df.rename(tobii_export_naming)],
                    how="horizontal",
                ).write_csv(outputdir / "joined.csv")
        except Exception as e:
            print(f"Unexpected exception {e = }, {type(e) = }")
            print(f"{outputdir = }")
